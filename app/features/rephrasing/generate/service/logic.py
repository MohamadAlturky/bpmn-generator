import os
from pathlib import Path
from dotenv import load_dotenv
from core.utils.text_between_marks_parser import extract_enclosed_string
from core.res.result import Result
from  ..models.types import QuestionDetails,GeneratedResult
from core.requests.providers.groq import GroqPromptSender
from core.models.message import Message,MessageHistory,add_message_to_history




#########################################
## read the prompts files  ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–  ##
#########################################

load_dotenv(override=True)
path = Path(__file__).parent.parent
task_path = os.path.join(path,"tasks/instructions/generate.prompt")
parser_path = os.path.join(path,"tasks/instructions/generate.format.prompt")

TASK_PROMPT = ""
FORMAT_PROMPT=""

with open(task_path, 'r') as file:
    TASK_PROMPT = file.read()

with open(parser_path, 'r') as file:
    FORMAT_PROMPT = file.read()
#â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸â˜ï¸#

# the service class for this slice
class Service:
    def __init__(self):
        pass

    def serve(self, request : QuestionDetails):
        sender = GroqPromptSender()
        PROMPT = TASK_PROMPT.replace("{process_description}",request.process_description)
        
        llm_history = MessageHistory(messages=[
            Message(role="user",content=PROMPT)
        ])  
        
        llm_response = sender.send_prompt_history(llm_history)
        print(llm_response)
        content = extract_enclosed_string(llm_response)
        if content == None:
            Result.failure("can't parse the result")
        return Result.success(GeneratedResult(process_description=content,
                                              notes=llm_response))